# FILE: app.py (FINAL VERSION WITH 2 IMPROVEMENTS)

import streamlit as st
from PIL import Image
import numpy as np
import cv2
import tempfile
from streamlit_webrtc import webrtc_streamer, VideoTransformerBase
import yaml
import torch
import mediapipe as mp
import pandas as pd
import time
import sys
import os

# --- TH√äM ƒê∆Ø·ªúNG D·∫™N C·ª¶A D·ª∞ √ÅN V√ÄO PYTHON PATH ---
sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), '.')))

# --- C√ÅC D√íNG IMPORT T√ôY CH·ªàNH ---
from src.model_hourglass.config import cfg
from src.model_hourglass.model.pose_network import PoseNetwork
from src.model_hourglass.util.vis_pose_only import draw_2d_skeleton
from src.model_mobilenet.handler import process_frame_with_mobilenet

# --- C√ÄI ƒê·∫∂T TRANG ---
st.set_page_config(page_title="Hand Pose Estimation Demo", page_icon="üëã", layout="wide")

# --- LOAD MODELS ---
@st.cache_resource
def load_posenetwork_model():
    """T·∫£i v√† c·∫•u h√¨nh m√¥ h√¨nh PoseNetwork (Hourglass)."""
    print("ƒêang t·∫£i model PoseNetwork...")
    config_file = "configs/eval_webcam.yaml"
    cfg.set_new_allowed(True)
    cfg.merge_from_file(config_file)
    cfg.freeze()
    model = PoseNetwork(cfg)
    device = cfg.MODEL.DEVICE
    model.load_model()
    model.to(device)
    model = model.eval()
    print("ƒê√£ t·∫£i model PoseNetwork th√†nh c√¥ng!")
    return model

@st.cache_resource
def load_mobilenet_model():
    """T·∫£i m√¥ h√¨nh MobileNetV2 (Caffe)."""
    print("ƒêang t·∫£i model MobileNetV2...")
    protoFile = "weights/mobilenet/pose_deploy.prototxt"
    weightsFile = "weights/mobilenet/pose_iter_102000.caffemodel"
    net = cv2.dnn.readNetFromCaffe(protoFile, weightsFile)
    print("ƒê√£ t·∫£i model MobileNetV2 th√†nh c√¥ng!")
    return net


posenetwork_model = load_posenetwork_model()
mobilenet_model = load_mobilenet_model()

# --- TI√äU ƒê·ªÄ V√Ä KH·ªûI T·∫†O MEDIAPIPE ---
st.title("So s√°nh c√°c m√¥ h√¨nh ∆Ø·ªõc t√≠nh T∆∞ th·∫ø B√†n tay")
mp_hands = mp.solutions.hands
mp_drawing = mp.solutions.drawing_utils
mp_drawing_styles = mp.solutions.drawing_styles

# --- C√ÅC H√ÄM X·ª¨ L√ù ---
def process_image_with_mediapipe(image_bgr):
    # (N·ªôi dung h√†m n√†y gi·ªØ nguy√™n)
    annotated_image = image_bgr.copy()
    with mp_hands.Hands(static_image_mode=True, max_num_hands=2, min_detection_confidence=0.5) as hands:
        image_rgb = cv2.cvtColor(image_bgr, cv2.COLOR_BGR2RGB)
        results = hands.process(image_rgb)
        if results.multi_hand_landmarks:
            for hand_landmarks in results.multi_hand_landmarks:
                mp_drawing.draw_landmarks(
                    annotated_image, hand_landmarks, mp_hands.HAND_CONNECTIONS,
                    mp_drawing_styles.get_default_hand_landmarks_style(),
                    mp_drawing_styles.get_default_hand_connections_style())
    return annotated_image

def process_with_posenetwork(image_bgr, model):
    annotated_image = image_bgr.copy()
    RESIZE_DIM = (256, 256)
    HEATMAP_DIM = (64, 64)
    with torch.no_grad():
        coord, _, est_pose_uv = model(annotated_image, detect_hand=True)
    if est_pose_uv is not None:
        est_pose_uv = est_pose_uv.to('cpu')
        crop_width = coord[2] - coord[0]
        crop_height = coord[3] - coord[1]
        scale_x = crop_width / HEATMAP_DIM[0]
        scale_y = crop_height / HEATMAP_DIM[1]
        keypoints_np = est_pose_uv[0].detach().numpy()
        keypoints_np[:, 0] = keypoints_np[:, 0] * scale_x + coord[0]
        keypoints_np[:, 1] = keypoints_np[:, 1] * scale_y + coord[1]
        cv2.rectangle(annotated_image, (coord[0], coord[1]), (coord[2], coord[3]), (0, 255, 0), 2)
        annotated_image = draw_2d_skeleton(annotated_image, keypoints_np)
    return annotated_image

def extract_posenetwork_coords(model, image_bgr):
    """Tr√≠ch xu·∫•t t·ªça ƒë·ªô th√¥ t·ª´ PoseNetwork v√† tr·∫£ v·ªÅ m·∫£ng Numpy."""
    with torch.no_grad():
        coord, _, est_pose_uv = model(image_bgr, detect_hand=True)
    if est_pose_uv is not None:
        est_pose_uv_cpu = est_pose_uv.to('cpu')
        HEATMAP_DIM = (64, 64)
        crop_width = coord[2] - coord[0]
        crop_height = coord[3] - coord[1]
        scale_x = crop_width / HEATMAP_DIM[0]
        scale_y = crop_height / HEATMAP_DIM[1]
        keypoints_np = est_pose_uv_cpu[0].detach().numpy()
        keypoints_np[:, 0] = keypoints_np[:, 0] * scale_x + coord[0]
        keypoints_np[:, 1] = keypoints_np[:, 1] * scale_y + coord[1]
        return keypoints_np
    return None

def extract_mobilenet_coords(net, image_bgr, threshold=0.1):
    """Tr√≠ch xu·∫•t t·ªça ƒë·ªô th√¥ t·ª´ MobileNet v√† tr·∫£ v·ªÅ m·∫£ng Numpy."""
    image_height, image_width, _ = image_bgr.shape
    aspect_ratio = image_width / image_height
    in_height = 368
    in_width = int(((aspect_ratio * in_height) * 8) // 8)
    inpBlob = cv2.dnn.blobFromImage(image_bgr, 1.0 / 255, (in_width, in_height), (0, 0, 0), swapRB=False, crop=False)
    net.setInput(inpBlob)
    output = net.forward()
    points = np.full((22, 2), np.nan, dtype=np.float32)
    for i in range(22):
        probMap = output[0, i, :, :]
        probMap = cv2.resize(probMap, (image_width, image_height))
        _, prob, _, point = cv2.minMaxLoc(probMap)
        if prob > threshold:
            points[i, 0], points[i, 1] = point[0], point[1]
    if np.all(np.isnan(points)): return None
    return points

def draw_posenetwork_skeleton(image, keypoints_np):
    """V·∫Ω b·ªô x∆∞∆°ng cho PoseNetwork."""
    if keypoints_np is None: return image
    return draw_2d_skeleton(image, keypoints_np)

def draw_mobilenet_skeleton(image, keypoints_np):
    """V·∫Ω b·ªô x∆∞∆°ng cho MobileNet."""
    if keypoints_np is None: return image
    POSE_PAIRS = [[0,1],[1,2],[2,3],[3,4],[0,5],[5,6],[6,7],[7,8],[0,9],[9,10],[10,11],[11,12],[0,13],[13,14],[14,15],[15,16],[0,17],[17,18],[18,19],[19,20]]
    for pair in POSE_PAIRS:
        partA, partB = pair[0], pair[1]
        if not np.isnan(keypoints_np[partA, 0]) and not np.isnan(keypoints_np[partB, 0]):
            ptA = (int(keypoints_np[partA, 0]), int(keypoints_np[partA, 1]))
            ptB = (int(keypoints_np[partB, 0]), int(keypoints_np[partB, 1]))
            cv2.line(image, ptA, ptB, (0, 255, 255), 3, lineType=cv2.LINE_AA)
            cv2.circle(image, ptA, 5, (0, 0, 255), thickness=-1, lineType=cv2.FILLED)
            cv2.circle(image, ptB, 5, (0, 0, 255), thickness=-1, lineType=cv2.FILLED)
    return image
    
# --- L·ªöP X·ª¨ L√ù WEBCAM (ƒê√É T√çCH H·ª¢P C·∫¢I TI·∫æN) ---
class HandPoseTransformer(VideoTransformerBase):
    def __init__(self):
        # Kh·ªüi t·∫°o c√°c thu·ªôc t√≠nh ·ªü tr·∫°ng th√°i "ch∆∞a s·∫µn s√†ng"
        self.model_choice = None
        self.posenetwork_model = None
        self.mobilenet_model = None
        self.threshold = 0.1
        self.last_processed_image = None # D√πng cho frame skipping

        # Kh·ªüi t·∫°o c√°c bi·∫øn thu th·∫≠p s·ªë li·ªáu
        self._reset_stats()

    def _reset_stats(self):
        """H√†m n·ªôi b·ªô ƒë·ªÉ reset t·∫•t c·∫£ c√°c s·ªë li·ªáu th·ªëng k√™."""
        self.frame_count = 0
        self.detection_count = 0
        self.jitter_values = []
        self.last_wrist_point = None
        self.fps_values = []
        self.prev_time = 0
        self.frame_counter_for_skipping = 0

    def update_config(self, model_choice, posenetwork_model_obj, mobilenet_model_obj, threshold, frame_skip_rate=0):
        # Reset s·ªë li·ªáu khi ƒë·ªïi model
        if self.model_choice != model_choice:
            self._reset_stats()
            print(f"Switched to model: {model_choice}. Resetting stats.")

        # C·∫≠p nh·∫≠t c√°c config
        self.model_choice = model_choice
        self.posenetwork_model = posenetwork_model_obj
        self.mobilenet_model = mobilenet_model_obj
        self.threshold = threshold
        # (Frame skipping s·∫Ω ƒë∆∞·ª£c x·ª≠ l√Ω trong logic ch√≠nh c·ªßa app)

    def transform(self, frame):
        img = frame.to_ndarray(format="bgr24")
        
        # Ki·ªÉm tra ƒëi·ªÅu ki·ªán s·∫µn s√†ng
        if not self.model_choice or \
           (self.model_choice == 'PoseNetwork (Hourglass)' and not self.posenetwork_model) or \
           (self.model_choice == 'MobileNetV2 (Lightweight)' and not self.mobilenet_model):
            return img

        # --- B·∫ÆT ƒê·∫¶U THU TH·∫¨P S·ªê LI·ªÜU ---
        self.frame_count += 1
        
        # T√≠nh to√°n FPS
        if self.prev_time > 0:
            try:
                fps = 1.0 / (time.time() - self.prev_time)
                self.fps_values.append(fps)
            except ZeroDivisionError:
                pass
        self.prev_time = time.time()
        
        # X·ª≠ l√Ω ch√≠nh
        processed_img = img.copy()
        raw_keypoints = None

        if self.model_choice == 'MediaPipe':
            processed_img = process_image_with_mediapipe(img)
        
        elif self.model_choice == 'PoseNetwork (Hourglass)':
            raw_keypoints = extract_posenetwork_coords(self.posenetwork_model, img)
            processed_img = draw_posenetwork_skeleton(img.copy(), raw_keypoints) if raw_keypoints is not None else img

        elif self.model_choice == 'MobileNetV2 (Lightweight)':
            raw_keypoints = extract_mobilenet_coords(self.mobilenet_model, img, self.threshold)
            processed_img = draw_mobilenet_skeleton(img.copy(), raw_keypoints) if raw_keypoints is not None else img

        # Thu th·∫≠p Jitter v√† Detection Count
        if raw_keypoints is not None:
            self.detection_count += 1
            wrist_point = raw_keypoints[0] # L·∫•y kh·ªõp c·ªï tay
            
            if self.last_wrist_point is not None and not np.isnan(wrist_point[0]) and not np.isnan(self.last_wrist_point[0]):
                jitter = np.linalg.norm(wrist_point - self.last_wrist_point)
                self.jitter_values.append(jitter)
            self.last_wrist_point = wrist_point
        
        return processed_img

# --- GIAO DI·ªÜN V√Ä LOGIC CH√çNH ---
with st.sidebar:
    st.title("T√πy ch·ªçn")
    source_choice = st.radio("Ch·ªçn Ngu·ªìn D·ªØ li·ªáu", ("Webcam Th·ªùi gian th·ª±c", "T·∫£i l√™n ·∫¢nh", "T·∫£i l√™n Video"))
    model_choice = st.radio(
        "Ch·ªçn Model", 
        ("MediaPipe", "PoseNetwork (Hourglass)", "MobileNetV2 (Lightweight)"),
        key="model_selector"
    )
    st.sidebar.header("C·∫£i ti·∫øn")
    confidence_threshold = st.sidebar.slider(
        "Ng∆∞·ª°ng tin c·∫≠y (MobileNetV2)", 
        min_value=0.0, max_value=1.0, value=0.1, step=0.05
    )

if source_choice == "T·∫£i l√™n ·∫¢nh":
    uploaded_file = st.file_uploader("Ch·ªçn m·ªôt file ·∫£nh...", type=["jpg", "png", "jpeg"])
    if uploaded_file:
        pil_image = Image.open(uploaded_file)
        cv_image = cv2.cvtColor(np.array(pil_image), cv2.COLOR_RGB2BGR)
        if model_choice == 'MediaPipe':
            processed_image = process_image_with_mediapipe(cv_image)
        elif model_choice == 'PoseNetwork (Hourglass)':
            processed_image = process_with_posenetwork(cv_image, posenetwork_model)
        else: # MobileNetV2
            processed_image = process_frame_with_mobilenet(mobilenet_model, cv_image, threshold=confidence_threshold)
        processed_image_rgb = cv2.cvtColor(processed_image, cv2.COLOR_BGR2RGB)
        col1, col2 = st.columns(2)
        with col1: st.image(pil_image, caption="·∫¢nh G·ªëc", use_column_width=True)
        with col2: st.image(processed_image_rgb, caption=f"K·∫øt qu·∫£ t·ª´: {model_choice}", use_column_width=True)

elif source_choice == "T·∫£i l√™n Video":
    uploaded_file = st.file_uploader("Ch·ªçn m·ªôt file video...", type=["mp4", "mov", "avi"])
    if uploaded_file is not None:
        tfile = tempfile.NamedTemporaryFile(delete=False)
        tfile.write(uploaded_file.read())
        cap = cv2.VideoCapture(tfile.name)
        st_frame = st.empty()
        while cap.isOpened():
            ret, frame = cap.read()
            if not ret: break
            if model_choice == 'MediaPipe':
                processed_frame = process_image_with_mediapipe(frame)
            elif model_choice == 'PoseNetwork (Hourglass)':
                processed_frame = process_with_posenetwork(frame, posenetwork_model)
            else: # MobileNetV2
                processed_frame = process_frame_with_mobilenet(mobilenet_model, frame, threshold=confidence_threshold)
            processed_frame_rgb = cv2.cvtColor(processed_frame, cv2.COLOR_BGR2RGB)
            st_frame.image(processed_frame_rgb, channels="RGB", use_column_width=True)
        cap.release()

elif source_choice == "Webcam Th·ªùi gian th·ª±c":
    st.header("Webcam Feed v√† Ph√¢n t√≠ch Th·ªùi gian th·ª±c")
    st.info("ƒê∆∞a tay c·ªßa b·∫°n tr∆∞·ªõc camera v√† gi·ªØ t∆∞∆°ng ƒë·ªëi y√™n trong v√†i gi√¢y ƒë·ªÉ ƒëo ƒë·ªô ·ªïn ƒë·ªãnh (Jitter).")
    
    # Chia layout
    video_col, stats_col = st.columns([3, 1])

    with video_col:
        ctx = webrtc_streamer(
            key="webcam",
            video_processor_factory=HandPoseTransformer,
            media_stream_constraints={"video": True, "audio": False},
            async_processing=True,
            rtc_configuration={"iceServers": [{"urls": ["stun:stun.l.google.com:19302"]}]}
        )

    with stats_col:
        st.subheader(f"Ph√¢n t√≠ch: {model_choice}")
        
        # T·∫°o c√°c placeholder
        fps_placeholder = st.empty()
        jitter_placeholder = st.empty()
        detection_placeholder = st.empty()
        chart_placeholder = st.empty()

        if ctx.video_processor:
            # C·∫≠p nh·∫≠t config cho worker
            ctx.video_processor.update_config(model_choice, posenetwork_model, mobilenet_model, confidence_threshold)

            # V√≤ng l·∫∑p ƒë·ªÉ c·∫≠p nh·∫≠t giao di·ªán s·ªë li·ªáu
            while ctx.state.playing:
                if model_choice != 'MediaPipe':
                    # L·∫•y d·ªØ li·ªáu t·ª´ worker
                    stats = {
                        "fps": np.mean(ctx.video_processor.fps_values) if ctx.video_processor.fps_values else 0,
                        "jitter": np.mean(ctx.video_processor.jitter_values) if ctx.video_processor.jitter_values else 0,
                        "detection_rate": (ctx.video_processor.detection_count / ctx.video_processor.frame_count * 100) if ctx.video_processor.frame_count > 0 else 0,
                        "fps_history": ctx.video_processor.fps_values
                    }

                    # C·∫≠p nh·∫≠t c√°c th·∫ª hi·ªÉn th·ªã
                    fps_placeholder.metric("FPS Trung b√¨nh", f"{stats['fps']:.1f}")
                    jitter_placeholder.metric("ƒê·ªô ·ªïn ƒë·ªãnh (Jitter)", f"{stats['jitter']:.2f} px/frame", help="C√†ng nh·ªè c√†ng t·ªët. ƒêo s·ª± rung gi·∫≠t c·ªßa kh·ªõp c·ªï tay.")
                    detection_placeholder.metric("T·ª∑ l·ªá Ph√°t hi·ªán", f"{stats['detection_rate']:.1f} %")

                    # C·∫≠p nh·∫≠t bi·ªÉu ƒë·ªì
                    if stats["fps_history"]:
                        chart_data = pd.DataFrame(stats["fps_history"], columns=["FPS"])
                        with chart_placeholder:
                            st.area_chart(chart_data.tail(100)) # D√πng area_chart cho ƒë·∫πp
                else:
                    # X√≥a s·ªë li·ªáu c≈© n·∫øu l√† MediaPipe
                    fps_placeholder.empty()
                    jitter_placeholder.empty()
                    detection_placeholder.empty()
                    chart_placeholder.empty()
                
                # Ch·ªù m·ªôt ch√∫t tr∆∞·ªõc khi c·∫≠p nh·∫≠t l·∫°i ƒë·ªÉ gi·∫£m t·∫£i
                time.sleep(0.5)
        else:
            st.info("Nh·∫•n 'START' tr√™n video feed ƒë·ªÉ b·∫Øt ƒë·∫ßu ph√¢n t√≠ch.")